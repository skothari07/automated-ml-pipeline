name: Scheduled Model Evaluation

on:
    schedule:
        - cron: "0 0 * * *"
    workflow_dispatch:

jobs:
    evaluate:
        runs-on: ubuntu-latest

        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.10"

            - name: Cache pip dependencies
              uses: actions/cache@v4
              with:
                  path: ~/.cache/pip
                  key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt

            - name: Download latest model artifact
              uses: dawidd6/action-download-artifact@v3
              with:
                  workflow: train_on_data_change.yml
                  name_is_regexp: true
                  name: trained-model-.*
                  path: models/
                  if_no_artifact_found: warn

            - name: Find model files
              id: find_model
              run: |
                  model_file=$(find models -name "model_*.joblib" -type f | head -1)

                  if [ -z "$model_file" ]; then
                    echo "No model found. Please train a model first."
                    echo "found=false" >> $GITHUB_OUTPUT
                    exit 0
                  fi

                  timestamp=$(echo $model_file | sed 's/.*model_\(.*\)\.joblib/\1/')
                  echo "timestamp=$timestamp" >> $GITHUB_OUTPUT
                  echo "found=true" >> $GITHUB_OUTPUT
                  echo "Model timestamp: $timestamp"

            - name: Run evaluation
              if: steps.find_model.outputs.found == 'true'
              run: |
                  echo "Running model evaluation..."
                  python src/evaluate_model.py --timestamp ${{ steps.find_model.outputs.timestamp }}

            - name: Upload evaluation report
              if: steps.find_model.outputs.found == 'true'
              uses: actions/upload-artifact@v4
              with:
                  name: evaluation-report-$(date +%Y%m%d)
                  path: metrics/evaluation_*.json
                  retention-days: 90

            - name: Create evaluation summary
              if: steps.find_model.outputs.found == 'true'
              run: |
                  echo "### Evaluation Report" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Date**: $(date '+%Y-%m-%d %H:%M:%S')" >> $GITHUB_STEP_SUMMARY
                  echo "**Model**: ${{ steps.find_model.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ -f metrics/evaluation_*.json ]; then
                    latest_eval=$(ls -t metrics/evaluation_*.json | head -1)
                    echo "**Results**:" >> $GITHUB_STEP_SUMMARY
                    echo '```json' >> $GITHUB_STEP_SUMMARY
                    cat $latest_eval >> $GITHUB_STEP_SUMMARY
                    echo '```' >> $GITHUB_STEP_SUMMARY
                  fi

            - name: No model found message
              if: steps.find_model.outputs.found == 'false'
              run: |
                  echo "### No Model Available" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "No trained model artifacts found." >> $GITHUB_STEP_SUMMARY
                  echo "Please run the training workflow first." >> $GITHUB_STEP_SUMMARY
